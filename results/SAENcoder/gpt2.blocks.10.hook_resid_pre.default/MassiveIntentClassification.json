{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 293.39715695381165,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.4684599865501008,
        "f1": 0.4592481919556016,
        "f1_weighted": 0.48256865953189687,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4684599865501008,
        "scores_per_experiment": [
          {
            "accuracy": 0.4616677874915938,
            "f1": 0.4625503885242007,
            "f1_weighted": 0.4700235851002586
          },
          {
            "accuracy": 0.4831876260928043,
            "f1": 0.47767037963706405,
            "f1_weighted": 0.492558560464768
          },
          {
            "accuracy": 0.4979825151311365,
            "f1": 0.4734668545949024,
            "f1_weighted": 0.5002605594175615
          },
          {
            "accuracy": 0.46099529253530597,
            "f1": 0.46067248670494937,
            "f1_weighted": 0.49820605384190664
          },
          {
            "accuracy": 0.4811701412239408,
            "f1": 0.457823656878621,
            "f1_weighted": 0.4949971875642702
          },
          {
            "accuracy": 0.47579018157363817,
            "f1": 0.4703217856335212,
            "f1_weighted": 0.4917868748967418
          },
          {
            "accuracy": 0.4589778076664425,
            "f1": 0.45313428802586647,
            "f1_weighted": 0.47522394874905527
          },
          {
            "accuracy": 0.44317417619367855,
            "f1": 0.43342647037577325,
            "f1_weighted": 0.45993740105074643
          },
          {
            "accuracy": 0.433759246805649,
            "f1": 0.4418799667885153,
            "f1_weighted": 0.44754294032985714
          },
          {
            "accuracy": 0.4878950907868191,
            "f1": 0.4615356423926027,
            "f1_weighted": 0.49514948390380314
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.4733398917855386,
        "f1": 0.45356174745027805,
        "f1_weighted": 0.48655608945236795,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4733398917855386,
        "scores_per_experiment": [
          {
            "accuracy": 0.47614363010329563,
            "f1": 0.4562499632537926,
            "f1_weighted": 0.48463186624610427
          },
          {
            "accuracy": 0.485489424495819,
            "f1": 0.4652678454302916,
            "f1_weighted": 0.4950050961910482
          },
          {
            "accuracy": 0.5061485489424495,
            "f1": 0.46384942299427107,
            "f1_weighted": 0.5112452457562007
          },
          {
            "accuracy": 0.45794392523364486,
            "f1": 0.4448452891826299,
            "f1_weighted": 0.4921371224148691
          },
          {
            "accuracy": 0.4899163797343827,
            "f1": 0.47267895421420664,
            "f1_weighted": 0.505533624436448
          },
          {
            "accuracy": 0.5007378258730939,
            "f1": 0.48572546918314086,
            "f1_weighted": 0.5138552231603165
          },
          {
            "accuracy": 0.4461387112641417,
            "f1": 0.4440584877561667,
            "f1_weighted": 0.4553504866497485
          },
          {
            "accuracy": 0.44220363994097395,
            "f1": 0.4126894000368537,
            "f1_weighted": 0.4511204195553196
          },
          {
            "accuracy": 0.4402361042793901,
            "f1": 0.4384696417316602,
            "f1_weighted": 0.4572409606396905
          },
          {
            "accuracy": 0.48844072798819477,
            "f1": 0.45178300071976696,
            "f1_weighted": 0.49944084947393425
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}